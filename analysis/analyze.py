#!/usr/bin/env python2

# The MIT License (MIT)
#
# Copyright (c) 2015 Noah Ollikainen, Samuel Thompson, Shane O'Connor
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

"""\
This script acts as a wrapper for the analysis scripts written by Noah Ollikainen. It accepts a target directory
containing a benchmark run and: i) calculates the metrics; and ii) creates the R figures.

input_directory specifies where the PDB files for analysis are contained and should meet the following criteria:
  * the results for each domain should be contained within separate folders e.g. PF00013_1WVN_A;
  * input_directory should contain a JSON file, benchmarks.json, which lists the different runs generated in the folder
    along with a Python regular expression used to select those files.
The scripts used to run the Rosetta protocols on the Sun Grid Engine cluster system meet these criteria so you will not
need to create the JSON files using that configuration. The JSON file is a mapping from benchmark descriptions to regular
expressions used to identify the output files. An example benchmarks.json can be found in this folder (benchmarks.json.example).

output_directory specifies the folder where the analysis files will be created.

Usage:
    analyze.py [options] <input_directory> <output_directory>

Options:

    --run RUNID -r RUNID
        Runs are defined in the benchmarks.json file. These values define a list of files which are then used as input
        for the analysis scripts. For example, in benchmarks.json.example, one valid argument to --run would be "kT = 0.9".
        If this option is omitted then analysis will be run for all benchmarks list in benchmarks.json.

    --overwrite
        If files generated during this analysis script (e.g. .fasta and .mi files) already exist then these are not
        regenerated by default. To force regeneration, use this flag.

    --expectn EXPECTED_CASES
        If a certain number of sequences is expected e.g. if 500 sequences were expected to be created then this flag
        can be used to ensure that no sequences are missing.

Authors:
    Noah Ollikainen: Analysis scripts
    Shane O'Connor: This script and modifications to the analysis scripts
"""

import collections
import contextlib
import math
import numpy
import os
import re
import shutil
import sys
import time
import json

from libraries import docopt
from libraries import colortext
from utils.fsio import read_file, write_file, get_file_lines
from utils.pdb import PDB
from utils.get_mi import get_domain_sequences, create_mi_file# compute_mi, get_domain_sequences, read_indices


def setup(input_directory, output_directory, run_ids):
    if not os.path.exists(input_directory):
        raise Exception('The input directory "{0}" does not exist.'.format(input_directory))
    benchmarks_file = os.path.join(input_directory, 'benchmarks.json')
    if not os.path.exists(benchmarks_file):
        raise Exception('The benchmark details file "{0}" does not exist. This needs to be created before proceeding with analysis.'.format(benchmarks_file))

    if not os.path.exists(output_directory):
        os.mkdir(output_directory)

    try:
        benchmark_runs = json.loads(read_file(benchmarks_file))
    except:
        raise Exception('The benchmark details file "{0}" does not have the correct format (JSON).'.format(benchmarks_file))
    if run_ids:
        missing_run_ids = set(run_ids).difference(benchmark_runs.keys())
        if missing_run_ids:
            raise Exception('The benchmark run(s) "%s" are not defined in the benchmark details file "%s".' % ('", "'.join(missing_run_ids), benchmarks_file))
        run_ids = sorted(run_ids)
    else:
        run_ids = sorted(benchmark_runs.keys())

    d = {}
    for run_id in run_ids:
        d[run_id] = benchmark_runs[run_id]
    return d


def get_normalized_run_file(run_id, extension):
    return re.sub("([^\w\d\-_~,;:\[\]\(\).])", "_", run_id) + extension


def create_fasta_file(run_id, input_directory, file_filter, overwrite_files, expectn):
    print('Folder: {0}'.format(os.path.split(input_directory)[1]))
    normalized_run_id = get_normalized_run_file(run_id, '.fasta')
    output_fasta_filepath = os.path.join(input_directory, normalized_run_id)
    if os.path.exists(output_fasta_filepath) and not overwrite_files:
        if expectn:
            num_fasta_headers = len([1 for l in get_file_lines(output_fasta_filepath) if l.startswith('>')])
            if expectn != num_fasta_headers:
                raise colortext.Exception('Expected {0} records in {1} but read {2}.'.format(expectn, output_fasta_filepath, num_fasta_headers))
        print('FASTA file exists. Skipping generation.' + input_directory)
        return output_fasta_filepath

    fasta_records = []
    c = 0
    tc = 0
    input_files = [os.path.abspath(os.path.join(input_directory, f)) for f in sorted(os.listdir(input_directory)) if re.match(file_filter, f)]
    progress_step = (len(input_files)/(20.0))
    if input_files:
        print('[' + ('=' * 6) + 'Progress' + ('=' * 6) + ']')
        sys.stdout.write(' ')
        for pdb_file in input_files:
            p = PDB.from_filepath(pdb_file)
            p.pdb_id = os.path.split(pdb_file)[1]
            if not (p.atom_sequences):
                raise Exception('Could not extract any ATOM sequence from the PDB file.')
            fasta_records.append(p.create_fasta(75).strip())
            c += 1
            tc += 1
            if c >= progress_step:
                c -= progress_step
                sys.stdout.write('.'); sys.stdout.flush()
        sys.stdout.write('\n'); sys.stdout.flush()
    if fasta_records:
        if expectn and (tc != expectn):
            raise colortext.Exception('Expected {0} records in {1} but read {2}.'.format(expectn, input_directory, tc))
        write_file(output_fasta_filepath, '\n'.join(fasta_records))
        print('{0} FASTA records written.'.format(len(fasta_records)))
        return output_fasta_filepath
    else:
        colortext.error('No FASTA files were created for directory "{0}".'.format(input_directory))
        return False



def analyze(input_directory, output_directory, run_ids, overwrite_files, expectn):
    benchmark_runs = setup(input_directory, output_directory, run_ids)

    domain_sequences = get_domain_sequences(read_file('domain_sequences.txt'))
    print(domain_sequences)
    for run_id, file_filter in sorted(benchmark_runs.iteritems()):
        colortext.message('\n== Creating input files for benchmark "{0}" ==\n'.format(run_id))
        colortext.message('Creating FASTA files'.format(run_id))
        for f in sorted(os.listdir(input_directory)):
            sub_dir = os.path.join(input_directory, f)
            if os.path.isdir(sub_dir):
                fasta_file = create_fasta_file(run_id, sub_dir, file_filter, overwrite_files, expectn)
                if fasta_file:
                    domain = os.path.split(sub_dir)[1].split('_')[0]
                    indices_directory = os.path.abspath('indices')
                    mutual_information_filepath = os.path.join(sub_dir, get_normalized_run_file(run_id, '.mi'))
                    if overwrite_files or not(os.path.exists(mutual_information_filepath)):
                        mi_file = create_mi_file(domain, read_file(fasta_file), domain_sequences, indices_directory, expectn = expectn)
                        write_file(mutual_information_filepath, mi_file)
                    else:
                        print('Mutual information (.mi) file exists. Skipping generation.' + input_directory)
                print('')





if __name__ == '__main__':

    try:
        arguments = docopt.docopt(__doc__.format(**locals()))
        input_directory = arguments['<input_directory>']
        output_directory = arguments['<output_directory>']
        run_ids = None
        overwrite_files = arguments['--overwrite']
        expectn = arguments['--expectn'] or None
        if expectn:
            try:
                assert(expectn.isdigit())
                expectn = int(expectn)
            except:
                raise Exception('The --expectn argument should be an integer.')
        if arguments['--run']:
            run_ids = [arguments['--run']]
        analyze(input_directory, output_directory, run_ids, overwrite_files, expectn)
    except KeyboardInterrupt:
        print